[
  {
    "seconds": 0,
    "slideNumber": 1,
    "shouldGenerateQuiz": true,
    "slideContent": {
      "title": "The Perceptron: Forward Propagation",
      "plain_text": [
        "Inputs x1, x2, ..., xm",
        "Weights w1, w2, ..., wm",
        "Summation (Σ) of weighted inputs",
        "Non-linearity via activation function (Ψ)",
        "Output ŷ and linear combination of inputs",
        "Infer broader neural network flow: inputs → weights → sum → non-linearity → output"
      ],
      "equations": [],
      "code": null,
      "diagrams": [],
      "graphs": [],
      "tables": []
    }
  },
  {
    "seconds": 145,
    "slideNumber": 2,
    "shouldGenerateQuiz": true,
    "slideContent": {
      "title": "Common Activation Functions",
      "plain_text": [
        "Sigmoid Function",
        "Hyperbolic Tangent (tanh)",
        "Rectified Linear Unit (ReLU)"
      ],
      "equations": [],
      "code": null,
      "diagrams": [],
      "graphs": [],
      "tables": []
    }
  },
  {
    "seconds": 237,
    "slideNumber": 3,
    "shouldGenerateQuiz": true,
    "slideContent": {
      "title": "The Perceptron: Example",
      "plain_text": [
        "Input layer with three features (x1, x2, and bias term w0 = 1)",
        "Weighted sums from inputs to a summation node (Σ) with weights [3, -2, 1] or corresponding connections",
        "Application of weights to inputs",
        "Activation via integration/sigmoid-like step function (represented by the next node)",
        "Output prediction ŷ (after activation)"
      ],
      "equations": [],
      "code": null,
      "diagrams": [],
      "graphs": [],
      "tables": []
    }
  },
  {
    "seconds": 415,
    "slideNumber": 4,
    "shouldGenerateQuiz": true,
    "slideContent": {
      "title": "The Perceptron: Example",
      "plain_text": [
        "Perceptron architecture",
        "Weighted summation of inputs",
        "Activation function (sigmoid/integration step)",
        "Forward pass: y_hat = g(1 + 3x1 - 2x2)",
        "Input vector example: X = [-1, 2]^T",
        "Decision boundary visualization (linear separator)",
        "Graphical depiction of decision boundary: line 1 + 3x1 - 2x2 = 0",
        "Note: example calculation y_hat = g(1 + 3*(-1) - 2*2) = g(-6) ≈ 0.002"
      ],
      "equations": [],
      "code": null,
      "diagrams": [],
      "graphs": [],
      "tables": []
    }
  },
  {
    "seconds": 507,
    "slideNumber": 5,
    "shouldGenerateQuiz": true,
    "slideContent": {
      "title": "The Perceptron: Simplified",
      "plain_text": [
        "Inputs x1, x2, ..., xm",
        "Weights w0, w1, ..., wm",
        "Summation node Σ",
        "Non-Linearity / Activation function",
        "Output ŷ"
      ],
      "equations": [],
      "code": null,
      "diagrams": [],
      "graphs": [],
      "tables": []
    }
  },
  {
    "seconds": 570,
    "slideNumber": 6,
    "shouldGenerateQuiz": true,
    "slideContent": {
      "title": "The Perceptron: Simplified",
      "plain_text": [
        "Inputs x1, x2, ..., xm",
        "Weights w1, w2, ..., wm",
        "Weighted sum z = w0 + sum_{j=1}^m x_j w_j",
        "Activation/output y = g(z)"
      ],
      "equations": [],
      "code": null,
      "diagrams": [],
      "graphs": [],
      "tables": []
    }
  },
  {
    "seconds": 615,
    "slideNumber": 7,
    "shouldGenerateQuiz": true,
    "slideContent": {
      "title": "Multi Output Perceptron",
      "plain_text": [
        "Structure of a multi-output perceptron",
        "Input layer with features x1, x2, ..., xm",
        "Hidden/processing to z1, z2 (pre-activation outputs)",
        "Activation/output functions: y1 = g(z1), y2 = g(z2)",
        "Feedforward weights: w0,i and wj,i in the summation",
        "Equation: zi = w0,i + sum_{j=1}^m x_j w_{j,i}"
      ],
      "equations": [],
      "code": null,
      "diagrams": [],
      "graphs": [],
      "tables": []
    }
  }
]